__author__ = ["Tomáš Ebert"]

import subprocess
import re
import requests
import os
from os import listdir
from os.path import isfile, join
import zipfile
from datetime import datetime
import shutil
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

COLLECTION = ""
LOADING_FILES = {
    "ThreatFox": "loading_files/threatfox/threatfox.txt",
    "URLHaus": "loading_files/urlhaus/urlhaus.txt",
    "Rescure": "loading_files/rescure/rescure.txt",
    "Steven Black": "loading_files/steven_black/steven_black.txt"
}
TEMP_FILES = {
    "URLHaus": "loading_files/urlhaus/urlhaus_temp.txt",
    "Rescure": "loading_files/rescure/rescure_temp.txt",
    "Steven Black": "loading_files/steven_black/steven_black_temp.txt"
}

class Loader:
    def __init__(self) -> None:
        self.result = None
        self.collection = COLLECTION
        self.loading_files = LOADING_FILES
        self.temp_files = TEMP_FILES

################################################################################################################################     
################################################################################################################################     

    def open_json(self, filename):
        with open(filename, "r") as domains_file:
            data = json.load(domains_file)
        return data

################################################################################################################################     
################################################################################################################################     

    def remove_IP_addresses(self, domain):
        # Remove IP addresses 
        if re.match(r'\d+.\d+.\d+.\d+:\d+', domain): 
            return True
        elif re.match(r'https?://\S+', domain):
            return True
        elif re.match(r'^[^.]*$', domain):
            return True
        return False

################################################################################################################################    
################################################################################################################################     
    
    def parse(self) -> None:
        url_urlhaus = "https://urlhaus.abuse.ch/downloads/hostfile/"
        url_rescure = "https://rescure.me/rescure_domain_blacklist.txt"
        url_steven_black = "https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts"
        urlhaus = "loading_files/urlhaus/urlhaus_temp.txt"
        rescure = "loading_files/rescure/rescure_temp.txt"
        steven_black = "loading_files/steven_black/steven_black_temp.txt"

        for source in self.loading_files:
            # Extract the directory part of the path
            directory = os.path.dirname(self.loading_files[source])
            # Create the directory if it doesn't exist
            os.makedirs(directory, exist_ok=True)
            
        # Make an HTTP request to the website
        response_urlhaus = requests.get(url_urlhaus)
        response_rescure = requests.get(url_rescure)
        response_steven_black  = requests.get(url_steven_black)

        # Define a regular expression pattern to match the domain
        pattern_urlhaus = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\s+([^\s]+)'
        pattern_steven_black = r'\b(?:[0]{1}\.){3}[0]{1}\s+([^\s]+)'
        
        # Remove lines starting with '#'
        response_rescure_text = re.sub(r'^\s*#.*\n?', '', response_rescure.text, flags=re.MULTILINE)
        response_rescure_text = response_rescure_text.split(" ")

        if response_urlhaus.status_code == response_rescure.status_code == response_steven_black.status_code == 200:
            # Extract the domain from the HTML content
            html_content_urlhaus = response_urlhaus.text
            html_content_steven_black= response_steven_black.text
            # Use re.search to find the match
            matches_urlhaus = re.findall(pattern_urlhaus, html_content_urlhaus)
            matches_steven_black = re.findall(pattern_steven_black, html_content_steven_black)

            print("Writing domains from URLHaus, Rescure and Steven Black to files...")
            # Extract and print each domain
            with open(urlhaus, "w") as urlhaus_file:
                for match in matches_urlhaus:
                    urlhaus_file.write(match + '\n')
                
            with open(rescure, "w") as rescure_file:
                for line in response_rescure_text:
                    rescure_file.write(line + '\n')

            with open(steven_black, "w") as steven_black_file:
                for line in matches_steven_black:
                    steven_black_file.write(line + '\n')

            current_date = datetime.now().strftime("%Y-%m-%d")

            # Copy the file and rename it
            shutil.copy2(urlhaus, f"loading_files/urlhaus/{current_date}.txt")
            shutil.copy2(rescure, f"loading_files/rescure/{current_date}.txt")
            shutil.copy2(steven_black, f"loading_files/steven_black/{current_date}.txt")
        else:
            if response_urlhaus.status_code != 200:
                print(f"URLHAUS failed with status code: {response_urlhaus.status_code}")
            if response_rescure.status_code != 200:
                print(f"RESCURE failed with status code: {response_rescure.status_code}")
            if response_steven_black.status_code != 200:
                print(f"STEVEN BLACK failed with status code: {response_steven_black.status_code}")

        print("Writing domains from URLHaus, Rescure and Steven Black to files completed!")

        print("Writing domains from ThreatFox to file...")
        self.result = subprocess.run(["python3", "threatfox.py"], text=True, check=True, universal_newlines=True)
        print("Writing domains from ThreatFox to file completed!")
        self.load_domains()

################################################################################################################################        
################################################################################################################################     

    def load_domains(self) -> None:
        print("Testing if the domains from ThreatFox are alive...")
        unique_domains = set() # For unique domains

        # List all files in the given directory
        directory = os.path.dirname(self.loading_files["ThreatFox"])
        files = os.listdir(directory)
        
        # Find the first .json file
        json_file = next((file for file in files if file.endswith('.json')), None)

        data = self.open_json(os.path.join(directory, json_file))
        if os.path.exists(self.loading_files["ThreatFox"]):
            # Reading old domains in that file for preventing duplicates from new ones
            with open(self.loading_files["ThreatFox"], "r") as old_domains:
                for domain in old_domains:
                    unique_domains.add(domain.strip())

        # Adding new domains
        with open(self.loading_files["ThreatFox"], "w") as domains:
            for key, value_list in data.items():
                for item in value_list:
                    if self.remove_IP_addresses(item["ioc"].strip()):
                        continue
                    if (self.is_alive(item["ioc"].strip())):
                        unique_domains.add(item["ioc"].strip())
            
            for index, domain in enumerate(unique_domains):
                if index == len(unique_domains) - 1:                
                    print(domain, file=domains, end="")
                    continue
                else:
                    print(domain, file=domains)
        print("Testing if the domains from ThreatFox are alive completed!")    

################################################################################################################################     
################################################################################################################################   

    def split(self, lines_per_file) -> None:
        print("Splitting Steven Black file into smaller parts for quicker alive testing...")
        steven_black = "loading_files/steven_black/steven_black_temp.txt"
        output_dir = "loading_files/steven_black/split_files"

        # Ensure the output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        with open(steven_black, 'r') as infile:
            lines = infile.readlines()
        
        total_lines = len(lines)
        total_files = (total_lines // lines_per_file) + (1 if total_lines % lines_per_file else 0)
        
        for i in range(total_files):
            start = i * lines_per_file
            end = start + lines_per_file
            out_lines = lines[start:end]
            
            output_file = os.path.join(output_dir, f'steven_black_part_{i+1}.txt')
            with open(output_file, 'w') as outfile:
                outfile.writelines(out_lines)
        
        print(f'Successfully split {total_lines} lines into {total_files} files.')
        print("Splitting Steven Black file into smaller parts for quicker alive testing completed!")

################################################################################################################################     
################################################################################################################################  

    def is_alive(self, domain) -> bool:
        try:
            command = ['ping', '-c', '1', domain] if subprocess.os.name != 'nt' else ['ping', '-n', '1', domain]
        
            # Run the ping command
            output = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

            return output.returncode == 0

        except Exception as e:
            print(f"An error occurred: {e}")
            return False

################################################################################################################################     
################################################################################################################################  

    def filter_alive_domains(self) -> None:
        urlhaus = "loading_files/urlhaus/urlhaus_temp.txt"
        rescure = "loading_files/rescure/rescure_temp.txt"
        steven_black_path = "loading_files/steven_black/split_files"
        steven_black = [join(steven_black_path, f) for f in listdir(steven_black_path) if isfile(join(steven_black_path, f))]
        alive_domains = []
        

        print("Testing if the domains from URLHaus are alive...")
        alive_domains = self.process_domains_from_file(urlhaus)
        with open(self.loading_files["URLHaus"], 'w') as file:
            for domain in alive_domains:
                file.write(domain + '\n')
        print("Testing if the domains from URLHaus are alive completed!")
        alive_domains = []


        print("Testing if the domains from Rescure are alive...")
        alive_domains = self.process_domains_from_file(rescure)
        with open(self.loading_files["Rescure"], 'w') as file:
            for domain in alive_domains:
                file.write(domain + '\n')
        print("Testing if the domains from Rescure are alive completed!")
        alive_domains = []


        print("Testing if the domains from Steven Black are alive...")
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            all_alive_domains = []
            # Prepare futures for each file
            futures = {
                executor.submit(self.process_domains_from_file, input_file): input_file
                for input_file in steven_black
            }
            # Collect results as they are completed
            for future in as_completed(futures):
                input_file = futures[future]
                try:
                    print(input_file)
                    alive_domains = future.result()  # Get the list of alive domains
                    all_alive_domains.extend(alive_domains)  # Aggregate alive domains
                except Exception as e:
                    print(f"An error occurred while processing {input_file}: {e}")

        with open(self.loading_files["Steven Black"], 'w') as file:
            for domain in alive_domains:
                file.write(domain + '\n')
        print("Testing if the domains from Steven Black are alive completed!")  

################################################################################################################################     
################################################################################################################################  

    def process_domains_from_file(self, input_file) -> list:
        with open(input_file, 'r') as file:
            domains = [line.strip() for line in file]

        alive_domains = []

        for domain in domains:
            if self.is_alive(domain):
                alive_domains.append(domain)

        return alive_domains
    
################################################################################################################################     
################################################################################################################################  

    def remove_files_and_folders(self) -> None:
        print("Removing temporary files...")
        os.remove(self.temp_files["URLHaus"])
        os.remove(self.temp_files["Rescure"])
        os.remove(self.temp_files["Steven Black"])
        shutil.rmtree("loading_files/steven_black/split_files")
        print("Removing temporary files completed!")

################################################################################################################################     
################################################################################################################################  

    def load(self) -> None:
        for source in self.loading_files:
            print(f"Loading domains into MongoDB collection from {source}...")
            self.result = subprocess.run(["python3", "../collector/main.py", "load", "-c"] + [self.collection] + ["-d"] +  [self.loading_files[source]] + ["-y"],
                                         text=True, universal_newlines=True)
            print(f"Loading domains into MongoDB collection from {source} completed!")

        for source in self.loading_files:
            # Extract the directory part of the path
            directory = os.path.dirname(self.loading_files[source])
            self.store_data(directory)

################################################################################################################################     
################################################################################################################################  

    def store_data(self, json_directory) -> None:
        for filename in os.listdir(json_directory):
            if filename.endswith('.json') or filename.endswith('.txt'):
                # Extract the year and month from the filename
                date_str = os.path.splitext(filename)[0]
                try:
                    date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                except ValueError:
                    # Skip the loading file
                    continue
                year = date_obj.year
                month = date_obj.month

                # Determine the year directory and ZIP file name
                year_directory = os.path.join(json_directory, str(year))
                zip_filename = os.path.join(year_directory, f'{month:02}.zip')

                # Ensure the year directory exists
                os.makedirs(year_directory, exist_ok=True)

                # Append the file to the ZIP archive
                file_path = os.path.join(json_directory, filename)
                with zipfile.ZipFile(zip_filename, 'a') as zipf:
                    zipf.write(file_path, os.path.basename(file_path))
                
                # Delete the original JSON file after adding it to the ZIP
                os.remove(file_path)             

################################################################################################################################     
################################################################################################################################  

if __name__ == "__main__":
    loader = Loader()
    loader.parse()
    loader.split(50)
    loader.filter_alive_domains()
    loader.load()
    loader.remove_files_and_folders()
    