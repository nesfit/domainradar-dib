# Domain Loading and Processing Script

## Overview

This script automates the process of downloading, processing, and storing domain data from multiple online sources. It performs tasks such as downloading domain lists, filtering out invalid or inactive domains, and loading the data into a MongoDB collection. The script also handles archiving of processed data into ZIP files organized by date.

## Features

- **Download Domain Data**: Fetches domain lists from various online sources, including URLHaus, Rescure, Steven Black, and ThreatFox.
- **Data Cleaning**: Removes invalid domains such as IP addresses and checks if domains are alive using a ping test.
- **Parallel Processing**: Uses multithreading to speed up the processing of large domain lists.
- **Data Archiving**: Archives processed data into ZIP files organized by year and month.
- **MongoDB Integration**: Loads the cleaned and processed domains into a MongoDB collection.

## Setup

### Prerequisites

- Python 3.x
- MongoDB
- Required Python libraries are from the collector + `requests`

### Installation

1. Clone the repository or download the script.
2.
```
python -m venv .venv
source .venv/bin/activate
python -m pip install -r requirements.txt
```
3. Ensure MongoDB is installed and running on your system.

## Directory Structure
The script expects a specific directory structure for loading and temporary files: 


loading_files/ \
│ \
├── threatfox/ \
│  $~~~~$ └── 202X/ $~~~~~~~~~~~~~~$# Folder with data of the X year in zip \
│  $~~~~$ └── threatfox.txt \
│ \
├── urlhaus/ \
│  $~~~~$ └── 202X/ $~~~~~~~~~~~~~~$# Folder with data of the X year in zip \
│  $~~~~$ └── urlhaus.txt \
│  $~~~~$ └── urlhaus_temp.txt \
│ \
├── rescure/ \
│  $~~~~$ └── 202X/ $~~~~~~~~~~~~~~$# Folder with data of the X year in zip \
│  $~~~~$ └── rescure.txt \
│  $~~~~$ └── rescure_temp.txt \
│ \
└── steven_black/ \
   $~~~~~~~$ └── 202X/ $~~~~~~~~~~~~~~$# Folder with data of the X year in zip \
   $~~~~~~~$ └── steven_black.txt \
   $~~~~~~~$ └── steven_black_temp.txt \
   $~~~~~~~$ └── split_files/   $~~~~~~~$ # Created dynamically during execution

The temporary files are for the direct loading from the source. 

## Configuration
The configuration for file paths and collection names can be found at the beginning of the script:
```python
COLLECTION = "YOUR COLLECTION"
LOADING_FILES = {
    "ThreatFox": "loading_files/threatfox/threatfox.txt",
    "URLHaus": "loading_files/urlhaus/urlhaus.txt",
    "Rescure": "loading_files/rescure/rescure.txt",
    "Steven Black": "loading_files/steven_black/steven_black.txt"
}
TEMP_FILES = {
    "URLHaus": "loading_files/urlhaus/urlhaus_temp.txt",
    "Rescure": "loading_files/rescure/rescure_temp.txt",
    "Steven Black": "loading_files/steven_black/steven_black_temp.txt"
}
```

## Usage
To execute the script, simply run the following command:
```bash
python3 loader.py
```

## Example Workflow

1. **Parse Domains**  
   The script downloads domain data from URLHaus, Rescure, and Steven Black, and stores it in temporary files. Also calls threatfox.py, which loads domains from ThreatFox.

2. **Split Large Files**  
   The Steven Black is such a big file so it is split into smaller chunks for parallel processing when testing livability of domains.

3. **Filter Active Domains**  
   The script pings each domain to check if it's alive and stores the result.

4. **Load into MongoDB**  
   The active domains are loaded into the MongoDB collection.

5. **Clean Up**  
   Temporary files and folders are deleted.

6. **Archive**  
    The file with the original loaded domains is named by the day, month, and year it was created and then archived in a zipped folder that is named by the month of the year. And these in turn are stored in the folders of the given years.


## Customization
- **Choosing Your Collection:** To store your data, you have to modify the `COLLECTION` for your collection in your database.
- **Adding New Sources:** To add a new domain source, modify the `LOADING_FILES` and `TEMP_FILES` dictionaries and update the `parse()` method to handle the new source.
- **Adjusting Parallel Processing:** The number of threads used in `filter_alive_domains()` can be adjusted by modifying the max_workers parameter in the ThreadPoolExecutor.
- **Splitting the files:** The bigger number of domains in splitted file in `split()` can reduce the number of splitted files of Steven Black. 